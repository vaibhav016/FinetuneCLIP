# dataset config
data: /path/to/your/folder
dataset: long_seq
dataset_shuffle_class_order: True
balanced_buffer: True
joint: False
buffer_size: 0
few_shot: 0
aircraft_buffer_size: 250
cifar100_buffer_size: 2000

# hyperparams related to tta
tta_epochs: 1
tta_phase: True
tta_loss_mode: "teacher_student"
# it can be eiher base(normal tta loss), or teacher_student
ema: True
oracle: False
schedule: 0.9999
tta_schedule: 0.9999
rd_loss: False

# base model and optimizer
model: 'ViT-B/16'
seed: 0
optimizer: adamw
lr: 7.5e-6
wd: 0.2
no_scheduler: False
epochs: 10
workers: 1
batch_size: 64
evaluation: False
sanity: False

#SPU config
k: 0.8
k_ttl: 0.9
scale: 1
update_all: True
select_loss_type: cn
cur_importance_batch_percentage: 1
sparsity: 0.1
score: norm
batchwise_spu_ttl: True
cosine_threshold: 0.1
use_2_mom_supervised: True 
use_sup_mask_in_ttl: False
compute_ttl_masks_every_batch: True
select_loss_type_ttl: cn
supervised_union_masks_per_task: True


# method
method: Finetune
# method: SPU

# device
device: cuda

name: CLIP
logs: logs
distributed: False
rank: 0
print_frequency: 10
val_frequency: 100
save_ckpt: False
save_base_path: logs
save_frequency: 100
resume: False
report_to: abcd
wandb_project_name: FinetuneCLIP
log_local: False
save_path: $SLURM_TMPDIR/output
#save_path: /Users/vaibhavsingh/Desktop/FinetuneCLIP/

debug: False
sweep: